searchState.loadedDescShard("parquet", 0, "This crate contains the official Native Rust …\nHigh-level API for reading/writing Arrow RecordBatches and …\nContains Rust mappings for Thrift definition. Refer to …\nBloom filter implementation specific to Parquet, as …\nLow level column reader and writer APIs.\nData types that connect Parquet physical types with their …\nCommon Parquet errors and macros.\nDefines a an item with an experimental public API\nAPIs for reading parquet data.\nAutomatically generated code from the Parquet thrift …\nContains record-based API for reading Parquet files.\nParquet schema definitions and methods to print and parse …\nCustom thrift definitions\nSchema metadata key used to store serialized Arrow IPC …\nSchema information necessary to decode a parquet file as …\nThe value of this metadata key, if present on …\nA <code>ProjectionMask</code> identifies a set of columns within a …\nCreate a <code>ProjectionMask</code> which selects all columns\nContains reader which reads parquet data into arrow …\nConvert arrow schema to parquet schema\nContains writer which writes arrow data into parquet data.\nProvides <code>async</code> API for reading parquet files as <code>RecordBatch</code>…\nContains async writer which writes arrow data into parquet …\nLogic for reading data into arrow buffers\nSpecialized decoders optimised for decoding to arrow format\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nReturns true if the leaf column <code>leaf_idx</code> is included by …\nCreate a <code>ProjectionMask</code> which selects only the specified …\nIf present a leaf column should be included if the value at\nLookups up the parquet column by name\nConvert a parquet <code>SchemaDescriptor</code> to <code>FieldLevels</code>\nConvert Parquet schema to Arrow schema including optional …\nConvert parquet schema to arrow schema including optional …\nCreate a <code>ProjectionMask</code> which selects only the specified …\nA predicate operating on <code>RecordBatch</code>\nAn <code>ArrowPredicate</code> created from an <code>FnMut</code>\nBuilder for constructing parquet readers into arrow.\nThe metadata necessary to construct a <code>ArrowReaderBuilder</code>\nOptions that control how metadata is read for a parquet …\nAn <code>Iterator&lt;Item = ArrowResult&lt;RecordBatch&gt;&gt;</code> that yields …\nA synchronous builder used to construct …\nFilter applied <em>during</em> the parquet read process\nA collection of row groups\n<code>RowSelection</code> allows selecting or skipping a provided …\n<code>RowSelection</code> is a collection of <code>RowSelector</code> used to skip …\nApplies an optional offset and limit to an optional …\nBuild a new <code>ParquetRecordBatchStream</code>\nBuild a <code>ParquetRecordBatchReader</code>\nBuild a <code>ParquetRecordBatchReader</code>\nReturns a <code>PageIterator</code> for the column chunks with the …\nEvaluate this predicate for the given <code>RecordBatch</code> …\nEvaluates an <code>ArrowPredicate</code>, returning a <code>RowSelection</code> …\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nRead bloom filter for a column in a row group Returns <code>None</code> …\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nLoads <code>ArrowReaderMetadata</code> from the provided <code>ChunkReader</code>, …\nReturns a new <code>ArrowReaderMetadata</code> for this builder\nReturns a reference to the <code>ParquetMetaData</code> for this …\nReturns a reference to the <code>ParquetMetaData</code> for this …\nThe Parquet Metadata, if known aprior\nCreate a new <code>ParquetRecordBatchStreamBuilder</code> with the …\nCreate a new <code>ParquetRecordBatchReader</code> that will read at …\nCreate a new <code>ArrowReaderOptions</code> with the default settings\nCreate a <code>ParquetRecordBatchStreamBuilder</code> from the provided …\nCreate a <code>ParquetRecordBatchReaderBuilder</code> from the provided …\nCreate a <code>ParquetRecordBatchReaderBuilder</code> from the provided …\nCreate a new <code>ParquetRecordBatchStreamBuilder</code> with the …\nGet the number of rows in this collection\nIf true, attempt to read <code>OffsetIndex</code> and <code>ColumnIndex</code>\nReturns the parquet <code>SchemaDescriptor</code> for this parquet file\nReturns the parquet <code>SchemaDescriptor</code> for this parquet file\nA list of <code>ArrowPredicate</code>\nReturns the <code>ProjectionMask</code> that describes the columns …\nThe number of rows\nOptional list of row group indices to scan\nReturns the arrow <code>SchemaRef</code> for this parquet file\nReturns the projected <code>SchemaRef</code> for reading the parquet …\nReturns the arrow <code>SchemaRef</code> for this parquet file\nThe Arrow Schema\nReturns <code>true</code> if <code>selection</code> is <code>None</code> or selects some rows\nIf true, skip <code>row_count</code> rows\nShould the reader strip any user defined metadata from the …\n<code>StatisticsConverter</code> to convert statistics in parquet …\nIf provided used as the schema for the file, otherwise the …\nCreate a new <code>ParquetRecordBatchReaderBuilder</code>\nCreate a new <code>ParquetRecordBatchReaderBuilder</code>\nCreate a new <code>ParquetRecordBatchReader</code> from the provided …\nCreate a new <code>ArrowReaderMetadata</code>\nCreate a new <code>ParquetRecordBatchReaderBuilder</code> with …\nCreate a new <code>ParquetRecordBatchReaderBuilder</code> with …\nCreate a new <code>ParquetRecordBatchReader</code> from the provided …\nSet the size of <code>RecordBatch</code> to produce. Defaults to 1024 …\nProvide a limit to the number of rows to be read\nProvide an offset to skip over the given number of rows\nEnable reading <code>PageIndex</code>, if present (defaults to <code>false</code>)\nOnly read data from the provided column indexes\nProvide a <code>RowFilter</code> to skip decoding rows\nOnly read data from the provided row group indexes\nProvide a <code>RowSelection</code> to filter out rows, and avoid …\nProvide a schema to use when reading the parquet file. If …\nSkip decoding the embedded arrow metadata (defaults to …\nA predicate operating on <code>RecordBatch</code>\nAn <code>ArrowPredicate</code> created from an <code>FnMut</code>\nFilter applied <em>during</em> the parquet read process\nEvaluate this predicate for the given <code>RecordBatch</code> …\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCreate a new <code>ArrowPredicateFn</code>. <code>f</code> will be passed batches …\nCreate a new <code>RowFilter</code> from an array of <code>ArrowPredicate</code>\nA list of <code>ArrowPredicate</code>\nReturns the <code>ProjectionMask</code> that describes the columns …\n<code>RowSelection</code> allows selecting or skipping a provided …\n<code>RowSelection</code> is a collection of <code>RowSelector</code> used to skip …\nreturns a <code>RowSelection</code> representing rows that are selected …\nReturns the argument unchanged.\nReturns the argument unchanged.\nCreates a <code>RowSelection</code> from an iterator of consecutive …\nCreates a <code>RowSelection</code> from a slice of <code>BooleanArray</code>\nCombine two lists of <code>RowSelection</code> return the intersection …\nCompute the intersection of two <code>RowSelection</code> For example: …\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nReturns an iterator over the <code>RowSelector</code>s for this …\nLimit this <code>RowSelection</code> to only select <code>limit</code> rows\nApplies an offset to this <code>RowSelection</code>, skipping the first …\nReturns the number of selected rows\nThe number of rows\nGiven an offset index, return the byte ranges for all data …\nSelect <code>row_count</code> rows\nReturns <code>true</code> if this <code>RowSelection</code> selects any rows\nSkip <code>row_count</code> rows\nIf true, skip <code>row_count</code> rows\nReturns the number of de-selected rows\nSplits off the first <code>row_count</code> from this <code>RowSelection</code>\nTrims this <code>RowSelection</code> removing any trailing skips\nCompute the union of two <code>RowSelection</code> For example: self:   …\nCombine two lists of <code>RowSelector</code> return the union of them …\nMaps an iterator of <code>ParquetStatistics</code> into an iterator of `…\nMaps an iterator of <code>ParquetStatistics</code> into an iterator of `…\nMaps an iterator of <code>ParquetStatistics</code> into an iterator of `…\nMaps an iterator of <code>ParquetStatistics</code> into an iterator of `…\nMaps an iterator of <code>ParquetStatistics</code> into an iterator of `…\nMaps an iterator of <code>ParquetStatistics</code> into an iterator of `…\nMaps an iterator of <code>ParquetStatistics</code> into an iterator of `…\nMaps an iterator of <code>ParquetStatistics</code> into an iterator of `…\nMaps an iterator of <code>ParquetStatistics</code> into an iterator of `…\nMaps an iterator of <code>ParquetStatistics</code> into an iterator of `…\nMaps an iterator of <code>ParquetStatistics</code> into an iterator of `…\nMaps an iterator of <code>ParquetStatistics</code> into an iterator of `…\nMaps an iterator of <code>ParquetStatistics</code> into an iterator of `…\nMaps an iterator of <code>ParquetStatistics</code> into an iterator of `…\nExtracts Parquet statistics as Arrow arrays\nReturn the arrow schema’s [<code>Field]</code> of the column in the …\nThe field (with data type) of the column in the Arrow …\nExtract the maximum values from Data Page statistics.\nExtract the minimum values from Data Page statistics.\nReturns a <code>UInt64Array</code> with null counts for each data page.\nReturns a <code>UInt64Array</code> with row counts for each data page.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nSpecial macro to combine the statistics iterators for min …\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nSpecial iterator adapter for extracting i128 values from …\nReturns a null array of data_type with one element per row …\nDefine an adapter iterator for extracting statistics from …\nExtracts the max statistics from an iterator of parquet …\nExtracts the max statistics from an iterator of …\nExtracts the min statistics from an iterator of parquet …\nExtracts the min statistics from an iterator of …\ntreat missing null_counts as 0 nulls\nCreate a new iterator to extract the statistics\nCreate a new iterator to extract the statistics\nCreate a new iterator to extract the statistics\nCreate a new iterator to extract the statistics\nCreate a new iterator to extract the statistics\nCreate a new iterator to extract the statistics\nCreate a new iterator to extract the statistics\nCreate a new iterator to extract the statistics\nCreate a new iterator to extract the statistics\nCreate a new iterator to extract the statistics\nCreate a new iterator to extract the statistics\nCreate a new iterator to extract the statistics\nCreate a new iterator to extract the statistics\nCreate a new iterator to extract the statistics\nreturn the next statistics value\nreturn the next statistics value\nreturn the next statistics value\nreturn the next statistics value\nreturn the next statistics value\nreturn the next statistics value\nreturn the next statistics value\nreturn the next statistics value\nreturn the next statistics value\nreturn the next statistics value\nreturn the next statistics value\nreturn the next statistics value\nreturn the next statistics value\nreturn the next statistics value\nExtracts the null count statistics from an iterator of …\nReturn the index of the column in the Parquet schema, if …\nthe index of the matched column in the Parquet schema\nExtract the maximum values from row group statistics in …\nExtract the minimum values from row group statistics in …\nExtract the null counts from row group statistics in …\nReturns a <code>UInt64Array</code> with row counts for each row group\nCreate a new <code>StatisticsConverter</code> to extract statistics for …\nSet the statistics converter to treat missing null counts …\nThe data for a single column chunk, see <code>ArrowColumnWriter</code>\nA single column chunk produced by <code>ArrowColumnWriter</code>\nA <code>Read</code> for <code>ArrowColumnChunkData</code>\nEncodes <code>ArrowLeafColumn</code> to <code>ArrowColumnChunk</code>\nA leaf column that can be encoded by <code>ArrowColumnWriter</code>\nEncodes <code>RecordBatch</code> to a parquet row group\nEncodes <code>RecordBatch</code> to parquet\nArrow-specific configuration settings for writing parquet …\nA shared <code>ArrowColumnChunkData</code>\nAdditional <code>KeyValue</code> metadata to be written in addition to …\nCalls <code>SerializedRowGroupWriter::append_column</code> with this …\nA copy of the Arrow schema.\nReturns the number of bytes written by this instance\nClose and finalize the underlying Parquet writer\nClose this column returning the written <code>ArrowColumnChunk</code>\nComputes the <code>ArrowLeafColumn</code> for a potentially nested …\nClose and finalize the underlying Parquet writer\nFlushes all buffered rows into a new row group\nReturns metadata for any flushed row groups\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGets the <code>ArrowColumnWriter</code> for the given <code>data_type</code>\nReturns the <code>ArrowColumnWriter</code> for a given schema\nReturns the estimated total encoded bytes for this column …\nReturns 12-byte values representing 3 values of months, …\nReturns 12-byte values representing 3 values of months, …\nThe in-progress row group if any\nReturns the number of rows buffered in the in progress row …\nAnticipated encoded size of the in progress row group.\nReturns a reference to the underlying writer.\nReturns a mutable reference to the underlying writer.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nFlushes any outstanding data and returns the underlying …\nParquet definition and repetition levels\nThe length of arrays to write to each row group\nEstimated memory usage, in bytes, of this <code>ArrowWriter</code>\nReturns the estimated total memory usage by the writer.\nCreates a new <code>ArrowWriterOptions</code> with the default settings.\nTry to create a new Arrow writer\nTry to create a new Arrow writer with <code>ArrowWriterOptions</code>.\nSets the <code>WriterProperties</code> for writing parquet files.\nSet the name of the root parquet schema element (defaults …\nSkip encoding the embedded arrow metadata (defaults to …\nEncodes the provided <code>RecordBatch</code>\nWrite an <code>ArrowLeafColumn</code>\nUnderlying Parquet writer\n<code>Storage</code> for the <code>Interner</code> used by <code>DictEncoder</code>\nA dictionary encoder for byte array data\nA fallback encoder, i.e. non-dictionary, for <code>ByteArray</code>\nThe fallback encoder in use\nComputes the min and max for the provided array and indices\nEncodes the provided <code>values</code> and <code>indices</code> to <code>encoder</code>\nEncode <code>values</code> to the in-progress page\nEncode <code>values</code> to the in-progress page\nReturns an estimate of the data page size in bytes\nReturns an estimate of the data page size in bytes\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCreate the fallback encoder for the given <code>ColumnDescPtr</code> …\nEncoded dictionary data\nThe data necessary to write a primitive Arrow array to …\nA fixed size list array\nA large list array\nThe definition and repetition level of an array within a …\nA helper to construct <code>ArrayLevels</code> from a potentially …\nA list array\nA primitive, leaf array\nA struct array\nThe arrow array\nPerforms a depth-first scan of the children of <code>array</code>, …\nThe current definition level\nArray’s definition levels\nFinish this <code>LevelInfoBuilder</code> returning the <code>ArrayLevels</code> for …\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nReturns true if the DataType can be represented as a …\nThe maximum definition level for this leaf column\nThe maximum repetition for this leaf column\nThe corresponding array identifying non-null slices of data\nThe current repetition level\nArray’s optional repetition levels\nCreate a new <code>LevelInfoBuilder</code> for the given <code>Field</code> and …\nVisits all children of this node in depth first order\nGiven an <code>array</code>, write the level data for the elements in …\nWrite <code>range</code> elements from FixedSizeListArray with child …\nWrite a primitive array, as defined by <code>is_leaf</code>\nWrite <code>range</code> elements from ListArray <code>array</code>\nWrite <code>range</code> elements from StructArray <code>array</code>\nThe asynchronous interface used by <code>ParquetRecordBatchStream</code>…\nAn in-memory column chunk\nImplements <code>PageIterator</code> for a single column chunk, …\nDecoding a batch\nFull column chunk and its offset\nContains the error value\nError\nAn in-memory collection of column chunks\nAt the start of a new row group, or the end of the parquet …\nA data source that can be used with <code>MetadataLoader</code> to load …\nAn asynchronous interface to load <code>ParquetMetaData</code> from an …\nContains the success value\nReads Parquet files in object storage using <code>ObjectStore</code>.\nAn asynchronous <code>Stream</code> of <code>RecordBatch</code> for a parquet file …\nA builder used to construct a <code>ParquetRecordBatchStream</code> for …\n<code>ReaderFactory</code> is used by <code>ParquetRecordBatchStream</code> to create\nReading data from input\nColumn chunk data representing only a subset of data pages\nBuild a new <code>ParquetRecordBatchStream</code>\nReturn a future that fetches the specified range of bytes …\nFetches the necessary column data into memory\nFunction that fetches byte ranges asynchronously\nFetches parquet metadata\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nRetrieve multiple byte ranges. The default implementation …\nRetrieve the bytes in <code>range</code>\nProvides asynchronous access to the <code>ParquetMetaData</code> of a …\nRead bloom filter for a column in a row group Returns <code>None</code> …\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nThe in-progress metadata\nCreate a new <code>ParquetRecordBatchStreamBuilder</code> with the …\nCreate a <code>ParquetRecordBatchStreamBuilder</code> from the provided …\nCreate a new <code>ParquetRecordBatchStreamBuilder</code> with the …\nReads the next row group with the provided <code>selection</code>, …\nThis is an option so it can be moved into a future\nThe offset and bytes of remaining unparsed data\nReturns the projected <code>SchemaRef</code> for reading the parquet …\nSet of data pages included in this sparse chunk. Each …\nLength of the full column chunk\nA data source that can be used with <code>MetadataLoader</code> to load …\nAn asynchronous interface to load <code>ParquetMetaData</code> from an …\nReturn a future that fetches the specified range of bytes …\nFunction that fetches byte ranges asynchronously\nFetches parquet metadata\nReturns the finished <code>ParquetMetaData</code>\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCreate a new <code>MetadataLoader</code> by reading the footer …\nLoads the page index, if any\nThe in-progress metadata\nCreate a new <code>MetadataLoader</code> from an existing …\nThe offset and bytes of remaining unparsed data\nReads Parquet files in object storage using <code>ObjectStore</code>.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCreates a new <code>ParquetObjectReader</code> for the provided …\nProvide a hint as to the size of the parquet file’s …\nLoad the Column Index as part of <code>Self::get_metadata</code>\nLoad the Offset Index as part of <code>Self::get_metadata</code>\nEncodes <code>RecordBatch</code> to parquet, outputting to an …\nThe asynchronous interface used by <code>AsyncArrowWriter</code> to …\n<code>ParquetObjectWriter</code> for writing to parquet to <code>ObjectStore</code>\nAppend <code>KeyValue</code> metadata in addition to those in …\nAsync writer provided by caller\nReturns the number of bytes written by this instance\nClose and finalize the writer.\nFlush any buffered data to the underlying writer and …\nFlush the data written by <code>sync_writer</code> into the <code>async_writer</code>\nClose and finalize the writer.\nFlushes all buffered rows into a new row group\nReturns metadata for any flushed row groups\nReturns the argument unchanged.\nReturns the number of rows buffered in the in progress row …\nAnticipated encoded size of the in progress row group.\nCalls <code>U::from(self)</code>.\nEstimated memory usage, in bytes, of this <code>ArrowWriter</code>\nUnderlying sync writer\nTry to create a new Async Arrow Writer\nTry to create a new Async Arrow Writer with …\nWrite the provided bytes to the underlying writer\nEnqueues the provided <code>RecordBatch</code> to be written\n<code>ParquetObjectWriter</code> for writing to parquet to <code>ObjectStore</code>\nReturns the argument unchanged.\nConstruct a new ParquetObjectWriter via a existing …\nCalls <code>U::from(self)</code>.\nConsume the writer and return the underlying BufWriter.\nCreate a new <code>ParquetObjectWriter</code> that writes to the …\nCounts the number of set bits in the provided range\nIterates through the set bit positions in <code>bytes</code> in reverse …\nPerforms big endian sign extension\nAn array of variable length byte arrays that are …\nReturns a mutable reference to a keys array\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nConverts this into an <code>ArrayRef</code> with the provided <code>data_type</code> …\nReturns a mutable reference to a values array\nA buffer of variable-sized byte arrays that can be …\nValidates that <code>&amp;self.values[start_offset..]</code> is a valid …\nExtends this buffer with a list of keys\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nConverts this into an <code>ArrayRef</code> with the provided <code>data_type</code> …\nReturns the number of byte arrays in this buffer\nIf <code>validate_utf8</code> this verifies that the first character of …\nA buffer of view type byte arrays that can be converted …\nDirectly append a view to the view array. This is used …\nSafety\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nConverts this into an <code>ArrayRef</code> with the provided <code>data_type</code> …\nDecoder for <code>Encoding::DELTA_BYTE_ARRAY</code>\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCreate a new <code>DeltaByteArrayDecoder</code> with the provided data …\nRead up to <code>len</code> values, returning the number of values read …\nReturns the number of values remaining\nSkip up to <code>to_skip</code> values, returning the number of values …\nDecoder for <code>Encoding::RLE_DICTIONARY</code> indices\nDecoder for the dictionary offsets array\nReturns the argument unchanged.\nWe want to decode the offsets in chunks so we will …\nCurrent length of <code>index_buf</code>\nCurrent offset into <code>index_buf</code>. If <code>index_buf_offset</code> == …\nCalls <code>U::from(self)</code>.\nThis is a maximum as the null count is not always known, …\nCreate a new <code>DictIndexDecoder</code> with the provided data page, …\nRead up to <code>len</code> values, returning the number of values read …\nSkip up to <code>to_skip</code> values, returning the number of values …\nA generic stateful column reader that delimits semantic …\nA <code>RecordReader</code> is a stateful column reader that delimits …\nReturns bitmap data for nullable columns. For non-nullable …\nReturns currently stored null bitmap data for nullable …\nReturns definition level data. The implementation has side …\nReturns currently stored buffer data. The side effect is …\nReturn repetition level data. The side effect is similar …\nThe decoder for the definition levels if any\nReturns the argument unchanged.\nTrue if the end of the current data page denotes the end …\nCalls <code>U::from(self)</code>.\nCreate a new <code>GenericRecordReader</code>\nThe total number of values stored in the data page.\nThe number of values from the current data page that has …\nReturns number of records stored in buffer.\nNumber of buffered records\nNumber of buffered records\nReturn number of values stored in buffer. If the parquet …\nNumber of buffered levels / null-padded values\nNumber of buffered levels / null-padded values\nReturns true if we do not need to unpack the nullability …\nTry to read one batch of data returning the number of …\nTry to read <code>num_records</code> of column data into internal …\nThe decoder for the repetition levels if any\nReset state of record reader. Should be called after …\nSet the current page reader.\nTry to skip the next <code>num_records</code> rows\nThe decoder for the values\nA buffer that supports padding with nulls\nIf a column contains nulls, more level data may be read …\nCompute levels and null mask\nOnly compute null bitmask - requires max level to be 1\nAn optimized decoder for decoding RLE and BIT_PACKED data …\nReturns the built null bitmask\nReturns the built level data\nDecodes a VLQ encoded little endian integer and returns it\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nThe length of this buffer\nSkips <code>level_num</code> definition levels\n<strong>Deprecated</strong> Bit-packed encoding.\nA boolean value.\nBrotli compression\nA BSON document embedded within a single BINARY column.\nArbitrary length byte array.\nEncoding for fixed-width data.\nRepresents a valid brotli compression level.\nA BSON document.\nColumn order that specifies what method was used to …\nSupported block compression algorithms.\nCommon types (converted types) used by frameworks when …\nData page Parquet 1.0\nData page Parquet 2.0\nA date stored as days since Unix epoch, encoded as the …\nA decimal value. This may be used to annotate binary or …\nDelta encoding for integers, either INT32 or INT64.\nIncremental encoding for byte arrays.\nEncoding for byte arrays to separate the length values and …\nDictionary page\nIEEE 754 double-precision floating point value.\nA date stored as days since Unix epoch.\nA decimal value with a specified scale and precision.\nAn enum is converted into a binary field\nEncodings supported by Parquet.\nA set of predefined values.\nFixed length byte array.\nIEEE 754 single-precision floating point value.\nA 16-bit floating point number.\nGzip compression\nRepresents a valid gzip compression level.\nIndex page\n32-bit signed integer.\n64-bit signed integer.\n96-bit signed integer for timestamps.\nAn interval of time.\nA signed 16 bit integer value stored as INT32 physical …\nA signed 32 bit integer value stored as INT32 physical …\nA signed 64 bit integer value stored as INT64 physical …\nA signed 8 bit integer value stored as INT32 physical type.\nAn integer with a specified bit width and signedness.\nA JSON document embedded within a single UTF8 column.\nA JSON document.\nA list is converted into an optional field containing a …\nLZ4 compression, (deprecated)\nLZ4 compression.\nLZO compression\nA list of elements.\nLogical types used by version 2.4.0+ of the Parquet format.\nA map is converted as an optional field containing a …\nA key/value pair is converted into a group of two fields.\nA map of key-value pairs.\nNo type conversion.\nField is optional (can be null) and each record has 0 or 1 …\nDefault byte encoding.\n<strong>Deprecated</strong> dictionary encoding.\nMirrors parquet::PageType\nField is repeated and can contain 0 or more values.\nField is required (can not be null) and each record has …\nGroup packed run length encoding.\nDictionary encoding.\nRepresentation of field types in schema.\nSigned (either value or legacy byte-wise) comparison.\nSnappy compression\nSort order for page and column statistics.\nA UTF8 encoded string.\nDate and time recorded as microseconds since the Unix …\nDate and time recorded as milliseconds since the Unix …\nThe total number of microseconds since midnight. The value …\nThe total number of milliseconds since midnight. The value …\nColumn uses the order defined by its logical or physical …\nA time stored as <code>TimeUnit</code> since midnight.\nA timestamp stored as <code>TimeUnit</code> since Unix epoch.\nTypes supported by Parquet.\nAn unsigned 16 bit integer value stored as INT32 physical …\nAn unsigned 32 bit integer value stored as INT32 physical …\nAn unsigned 64 bit integer value stored as INT64 physical …\nAn unsigned 8 bit integer value stored as INT32 physical …\nNo compression.\nComparison is undefined.\nUndefined column order, means legacy behaviour before …\nUnsigned (depending on physical type either value or …\nA BYTE_ARRAY actually contains UTF8 encoded chars.\nAn unknown logical type.\nA UUID.\nZSTD compression\nRepresents a valid zstd compression level.\nReturns the codec type of this compression setting as a …\nReturns the compression level.\nReturns the compression level.\nReturns the compression level.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns default sort order based on physical type.\nReturns sort order for a physical/logical type.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nReturns true if this is <code>Self::SIGNED</code>\nReturns sort order associated with this column order.\nAttempts to create a gzip compression level.\nAttempts to create a brotli compression level.\nAttempts to create a zstd compression level from a given …\nThe number of bits in the integer.\nWhether the time is adjusted to UTC.\nWhether the timestamp is adjusted to UTC.\nWhether the integer is signed.\nThe location of the decimal point.\nThe number of digits in the decimal.\nThe unit of time.\nThe unit of time.\nEach block is 256 bits, broken up into eight contiguous “…\nSalt as defined in the spec.\nA split block Bloom filter.\nreturns true when every bit that is set in the result of …\nCheck if an AsBytes value is probably present or …\nCheck if a hash is in the filter. May return true for …\ngiven an initial offset, and a byte buffer, try to read …\nReturn the total in memory size of this bloom filter in …\nReturns the argument unchanged.\nReturns the argument unchanged.\nCreate and populate <code>BloomFilterHeader</code> from this bitset for …\nsetting every bit in the block that was also set in the …\nInsert an AsBytes value into the filter\nInsert a hash into the filter\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\ntakes as its argument a single unsigned 32-bit integer and …\nCreate a new Sbbf with given number of distinct values and …\nCreate a new Sbbf with given number of bytes, the exact …\ngiven a Bytes buffer, try to read out a bloom filter …\nRead a new bloom filter from the given offset in the given …\nWrite the bloom filter data (header and then bitset) to …\nWrite the bitset in serialized form to the writer.\nContains Parquet Page definitions and page reader …\nContains column reader API.\nContains column writer API.\nHelper struct to represent pages with potentially …\nData page Parquet format v1.\nData page Parquet format v2.\nDictionary page.\nParquet Page definition.\nAn iterator over pages of one specific column in a parquet …\nContains metadata for a page\nAPI for reading pages from a column chunk. This offers a …\nContains page write metrics.\nAPI for writing pages in a column chunk.\nReturns <code>true</code> if the next page can be assumed to contain …\nReturns internal byte buffer reference for this page.\nThe number of bytes written to the underlying sink\nCloses resources and flushes underlying sink. Page writer …\nReturns underlying page with potentially compressed buffer.\nReturns compressed size in bytes.\nThe compressed size of the page\nReturns slice of compressed buffer in the page.\nReturns encoding for values in page.\nReturns this page <code>Encoding</code>.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGets the next page in the column chunk associated with …\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nReturns true if the page is a dictionary page\nCreates <code>CompressedPage</code> from a page with potentially …\nCreates new spec with default page write metrics.\nThe number of levels within the page if known\nThe number of rows within the page if known\nNumber of values in page.\nReturns number of values in this page.\nThe number of values in the page\nThe offset of the page in the column chunk\nReturns page type.\nReturns <code>PageType</code> for this page.\nThe type of page being written\nGets metadata about the next page, returns an error if no …\nSkips reading the next page, returns an error if no column …\nReturns optional <code>Statistics</code>.\nReturns the thrift page header\nReturns uncompressed size in bytes.\nThe total size of the page, before compression\nWrites a page into the output stream/sink. Returns …\nThe underlying data buffer\nThe underlying data buffer\nThe underlying data buffer\nDefinition level encoding\nLength of definition levels\nEncoding for values in this page\nEncoding for values in this page\nEncoding for values in this page\nIs this page compressed\nIs dictionary page sorted\nNumber of null values in this page\nNumber of rows in this page\nNumber of values in this page\nNumber of values in this page\nNumber of values in this page\nRepetition level encoding\nLength of repetition levels\nOptional statistics for this page\nOptional statistics for this page\nColumn reader for boolean type\nColumn reader for byte array type\nColumn reader for a Parquet type.\nTyped value reader for a particular primitive column.\nColumn reader for double type\nColumn reader for fixed length byte array type\nColumn reader for float type\nReads data for a given column chunk, using the provided …\nColumn reader for int32 type\nColumn reader for int64 type\nColumn reader for int96 type\nThe decoder for the definition levels if any\nThe decoder for the definition levels if any\nReturns the argument unchanged.\nReturns the argument unchanged.\nGets a specific column reader corresponding to column …\nGets a typed column reader for the specific type <code>T</code>, by “…\nCheck whether there is more data to read from this column, …\nTrue if the end of the current data page denotes the end …\nTrue if the end of the current data page denotes the end …\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCreates new column reader based on column descriptor and …\nThe total number of values stored in the data page.\nThe total number of values stored in the data page.\nThe number of values from the current data page that has …\nThe number of values from the current data page that has …\nReads a batch of values of at most <code>batch_size</code>, returning a …\nRead the next page as a dictionary page. If the next page …\nReads a new page and set up the decoders for levels, …\nRead up to <code>max_records</code> whole records, returning the number …\nThe decoder for the repetition levels if any\nThe decoder for the repetition levels if any\nSkips over <code>num_records</code> records, where records are …\nThe decoder for the values\nThe decoder for the values\nDecodes level data\nDecodes value data\nAn implementation of <code>ColumnValueDecoder</code> for <code>[T::T]</code>\nAn implementation of <code>DefinitionLevelDecoder</code> for <code>[i16]</code>\nAn implementation of <code>RepetitionLevelDecoder</code> for <code>[i16]</code>\nInspects the buffered repetition levels in the range …\nFlush any partially read or skipped record\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCreate a new <code>ColumnValueDecoder</code>\nRead up to <code>num_values</code> values into <code>out</code>\nRead up to <code>num_levels</code> definition levels into <code>out</code>\nRead up to <code>max_records</code> of repetition level data into <code>out</code> …\nSet data for this <code>ColumnLevelDecoder</code>\nSet the current data page\nSet the current dictionary page\nSkips over <code>num_levels</code> definition levels\nSkips over up to <code>num_levels</code> repetition levels …\nSkips over <code>num_values</code> values\nColumn writer for boolean type\nColumn writer for byte array type\nMetadata returned by <code>GenericColumnWriter::close</code>\nColumn writer for a Parquet type.\nTyped column writer for a primitive column.\nColumn writer for double type\nColumn writer for fixed length byte array type\nColumn writer for float type\nGeneric column writer for a primitive column.\nColumn writer for int32 type\nColumn writer for int64 type\nColumn writer for int96 (timestamp) type\nAdds data page. Data page is either buffered in case of …\nOptional bloom filter for this column\nAssembles column chunk metadata.\nThe total number of bytes written\nDetermine if we should allow truncating min/max values for …\nClose this <code>ColumnWriter</code>\nFinalizes writes and closes the column writer. Returns …\nOptional column index, for filtering\nEvaluate <code>a &gt; b</code> according to underlying logical type.\nSigned comparison of bytes arrays\nPerforms dictionary fallback. Prepares and writes …\nEncodes definition or repetition levels for Data Page v1.\nEncodes definition or repetition levels for Data Page v2. …\nThe order of encodings within the generated metadata does …\nThe order of encodings within the generated metadata does …\nReturns encoding for a column when no other encoding is …\nFinalises any outstanding data pages and flushes buffered …\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGets a specific column writer corresponding to column …\nReturns a reference to a <code>ColumnDescPtr</code>\nReturns the estimated total encoded bytes for this column …\nReturns the estimated total encoded bytes for this column …\nReturns total number of bytes written by this column …\nReturns total number of rows written by this column writer …\nGets a typed column writer for the specific type <code>T</code>, by “…\nSimilar to <code>get_typed_column_writer</code> but returns a reference.\nSimilar to <code>get_typed_column_writer</code> but returns a reference.\nReturns true if dictionary is supported for column writer, …\nTry and increment the bytes from right to left.\nTry and increment the the string’s bytes from right to …\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\n(min, max)\n(min, max)\nReturns the estimated total memory usage\nReturns the estimated total memory usage.\nMetadata for this column chunk\nReturns a new instance of <code>GenericColumnWriter</code>.\nResets the state of this <code>PageMetrics</code> to the initial state. …\nOptional offset index, identifying page locations\nThe total number of rows written\nReturns true if there is enough data for a data page, …\nReturns true if we need to fall back to non-dictionary …\nTruncate a UTF8 slice to the longest prefix that is still …\nUpdate the column index and offset index when adding the …\nUpdates histogram values using provided definition levels\nSum the provided PageMetrics histograms into the chunk …\nSum <code>page_histogram</code> into <code>chunk_histogram</code>\nUpdates column writer metrics with each page metadata.\nUpdates histogram values using provided repetition levels\nPerform a conditional update of <code>cur</code>, skipping any NaN …\nSum the provided page variable_length_bytes into the chunk …\nInitialize the definition level histogram\nInitialize the definition level histogram\nInitialize the repetition level histogram\nInitialize the repetition level histogram\nWrites batch of values, definition levels and repetition …\nWriter may optionally provide pre-calculated statistics …\nWrites compressed data page into underlying sink and …\nWrites dictionary page into underlying sink.\nWrites mini batch of values, definition and repetition …\nA generic encoder of <code>ColumnValues</code> to data and dictionary …\nA collection of <code>ParquetValueType</code> encoded by a …\nThe encoded values for a data page, with optional …\nThe encoded data for a dictionary page\nThe underlying value type of <code>Self::Values</code>\nThe values encoded by this encoder\nReturns an estimate of the encoded data page size in bytes\nReturns an estimate of the encoded size of dictionary page …\nReturns the estimated total memory usage of the encoder\nFlushes bloom filter if enabled and returns it, otherwise …\nFlush the next data page for this column chunk\nFlush the dictionary page for this column chunk if any. …\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns true if this encoder has a dictionary page\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nThe number of values in this collection\nReturns the number of buffered values\nCreate a new <code>ColumnValueEncoder</code>\nWrite the corresponding values to this <code>ColumnValueEncoder</code>\nWrite the values at the indexes in <code>indices</code> to this …\nConverts an instance of data type to a slice of bytes as <code>u8</code>…\nParquet physical type: BoolType\nRust representation for BYTE_ARRAY and …\nParquet physical type: ByteArrayType\nDecimal backed by byte array.\nContains the Parquet physical type information as well as …\nRust representation for Decimal values.\nParquet physical type: DoubleType\nWrapper type for performance reasons, this represents …\nParquet physical type: FixedLenByteArrayType\nParquet physical type: FloatType\nDecimal backed by <code>i32</code>.\nParquet physical type: Int32Type\nDecimal backed by <code>i64</code>.\nParquet physical type: Int64Type\nRust representation for logical type INT96, value is …\nParquet physical type: Int96Type\nConverts an slice of a data type to a slice of bytes.\nThe physical type of the Parquet data type.\nReturns slice of bytes for this data type.\nTry to convert the byte array to a utf8 slice\nReturns underlying data as slice of <code>u32</code>.\nReturns slice of data.\nReturns bytes of unscaled value.\nMacro to reduce repetition in making type assertions on …\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCreates new decimal value from <code>ByteArray</code>.\nCreates new decimal value from <code>i32</code>.\nCreates new decimal value from <code>i64</code>.\nReturns the underlying <code>ColumnReaderImpl</code> for the given …\nReturns the underlying <code>ColumnWriterImpl</code> for the given …\nReturns a mutable reference to the underlying …\nReturns a reference to the underlying <code>ColumnWriterImpl</code> for …\nReturns Parquet physical type.\nReturns size in bytes for Rust representation of the …\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nChecks if the underlying buffer is empty.\nGets length of the underlying byte buffer.\nCreates new INT96 type struct with no data set.\nCreates new byte array with no data set.\nReturns decimal precision.\nReturns decimal scale.\nSets data for this INT96 type.\nSet data from another byte buffer.\nReturns <code>ByteArray</code> instance with slice of values for a data.\nReturns slice of bytes for a slice of this data type.\nReturn the internal representation as a mutable slice\nConverts this INT96 into an i64 representing the number of …\nConverts this INT96 into an i64 representing the number of …\nConverts this INT96 to a number of seconds and nanoseconds …\nThe total number of digits in the number\nThe total number of digits in the number\nThe total number of digits in the number\nThe number of digits to the right of the decimal point\nThe number of digits to the right of the decimal point\nThe number of digits to the right of the decimal point\nThe underlying value\nThe underlying value\nThe underlying value\nSealed trait to start to remove specialisation from …\nReturn the value as an Any to allow for downcasts without …\nReturn the value as i64 if possible\nReturn the value as an mutable Any to allow for downcasts …\nReturn the value as u64 if possible\nDecode the value from a given buffer for a higher level …\nReturn the encoded size for a type\nEncode the value directly from a higher level encoder\nEstablish the data that will be decoded in a buffer\nSets the value of this object from the provided <code>Bytes</code>\nReturn the number of variable length bytes in a given …\nArrow error. Returned when reading into arrow or writing …\n“End of file” Parquet error. Returned when IO related …\nContains the error value\nAn external error variant\nGeneral Parquet error. Returned when code violates normal …\nError when the requested column index is more than the …\n“Not yet implemented” Parquet error. Returned when …\nContains the success value\nParquet error enumeration\nA specialized <code>Result</code> for Parquet errors.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nThe length of the parquet footer in bytes\nModule for working with Parquet file footers.\nParquet metadata API\nPer-page encoding information.\nPage Index of “Column Index Layout to Support Page …\nConfiguration via <code>WriterProperties</code> and <code>ReaderProperties</code>\nFile reader API and methods to access file metadata, row …\nContains implementations of the reader traits FileReader, …\nContains definitions for working with Parquet statistics.\nContains file writer API, and provides methods to write …\nDecodes the Parquet footer returning the metadata length …\nDecodes <code>ParquetMetaData</code> from the provided bytes.\nReads the ParquetMetaData from the footer of the parquet …\nMetadata for a column chunk.\nBuilder for <code>ColumnChunkMetaData</code>\nBuilder for Parquet <code>ColumnIndex</code>, part of the Parquet …\nFile level metadata for a Parquet file.\nReference counted pointer for <code>FileMetaData</code>.\nA key-value pair for <code>FileMetaData</code>.\nHistograms for repetition and definition levels.\nBuilder for offset index, part of the Parquet PageIndex.\nPage level statistics for each column chunk of each row …\nParsed metadata for a single Parquet file\nA builder for creating / manipulating <code>ParquetMetaData</code>\nReads the <code>ParquetMetaData</code> from a byte stream.\nWrites <code>ParquetMetaData</code> to a byte stream\n<code>OffsetIndexMetaData</code> for each data page of each row group …\nMetadata for a row group\nBuilder for row group metadata.\nReference counted pointer for <code>RowGroupMetaData</code>.\nAdds the values from the other histogram to this histogram\nAdds a column metadata to this row group\nAdds a row group to the metadata\nAppend statistics for the next page\nAppend the given page-level histograms to the <code>ColumnIndex</code> …\nAppend the offset and size of the next page.\nAppend the row count of the next page.\nAppend the unencoded byte array data bytes of the next …\nReturns the offset for the bloom filter.\nReturns the offset for the bloom filter.\nCreates a new ParquetMetaData from the builder\nBuilds row group metadata.\nBuilds column chunk metadata.\nBuild and get the thrift metadata of column index\nBuild and get the thrift metadata of offset index\nReturns builder for row group metadata.\nReturns builder for column chunk metadata.\nReturns the offset and length in bytes of the column chunk …\nClears the page encoding stats for this column chunk.\nClears the statistics for this column chunk.\nReturns column chunk metadata for <code>i</code>th column.\nDescriptor for this column.\nReference counted clone of descriptor for this column.\nReturn a reference to the current column index, if any\nReturns the column index for this file if loaded\nPage level index for each page in each column chunk\nReturns the offset for the column index length.\nReturns the offset for the column index.\nReturns the range for the offset index if any\nReturns column order for <code>i</code>th column in this file. If …\nColumn (sort) order used for <code>min</code> and <code>max</code> values of each …\nPath (or identifier) of this column.\nType of this column. Must be primitive.\nReturns slice of column chunk metadata.\nReturns mutable slice of column chunk metadata.\nTotal size of all compressed column data in this row group.\nReturns the total compressed data size of this column …\nCompression for this column.\nString message for application that wrote this file.\nReturns the offset for the column data.\nReturns the definition level histogram.\ncontains the concatenation of the histograms of all pages\nReturns the offset for the dictionary page, if any.\nAll encodings used for this column.\nReturns file metadata as reference.\nFile level metadata\nReturns file offset of this row group in file.\nByte offset of <code>ColumnMetaData</code> in <code>file_path()</code>.\nWe can’t infer from file offset of first column since …\nFile where the column chunk is stored.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nMethod to convert from Thrift.\nMethod to convert from Thrift.\nReturns the histogram value at the given index.\nReturns the offset for the index page.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nConvert this ParquetMetaData into a <code>ParquetMetaDataBuilder</code>\nConverts this <code>RowGroupMetaData</code> into a …\nConverts this <code>ColumnChunkMetaData</code> into a …\nReturn the inner vector, consuming self\nreturns if the histogram is empty\nReturns key_value_metadata of this file.\nreturn the length of the histogram\nMemory calculations for <code>ParquetMetadata::memory_size</code>\nEstimate of the bytes allocated to store <code>ParquetMetadata</code>\nCreate a new builder from a file metadata, with no row …\nCreates new builder from schema descriptor.\nCreates new column chunk metadata builder.\nCreates a new column index builder.\nCreates a new offset index builder.\nCreates Parquet metadata from file metadata and a list of …\nCreates new file metadata.\nCreate a new builder from an existing ParquetMetaData\nCreates Parquet metadata from file metadata, a list of row …\nNumber of columns in this row group.\nReturns number of row groups in this file.\nReturns number of rows in the file.\nNumber of rows in this row group.\nTotal number of values in this column chunk.\nReturn a reference to the current offset index, if any\nReturns offset indexes in this file, if loaded\nOffset index for each page in each column chunk\nReturns the offset for the offset index length.\nReturns the offset for the offset index.\nReturns the range for the offset index if any\nReturns the offset index for this file if loaded\nReturns ordinal position of this row group in file.\nOrdinal position of this row group in file\nReturns the offset for the page encoding stats, or <code>None</code> if …\nReturns page indexes in this file.\nReturns the repetition level histogram.\ncontains the concatenation of the histograms of all pages\nSets the values of all histogram levels to 0.\nReturns row group metadata for <code>i</code>th position. Position …\nReturn a reference to the current row groups\nReturns slice of row groups in this file.\nRow group metadata\nReturns Parquet <code>Type</code> that describes schema in this file.\nReturns a reference to schema descriptor.\nReturns reference to a schema descriptor.\nReturns reference counted clone for schema descriptor.\nReturns reference counted clone of schema descriptor.\nSets optional bloom filter length in bytes.\nSets optional bloom filter offset in bytes.\nSet the boundary order of the column index\nSets the column index\nOverride the column index\nSets optional column index length in bytes.\nSets optional column index offset in bytes.\nSets column metadata for this row group.\nSets compression.\nSets data page offset in bytes.\nSets optional repetition level histogram\nSets optional dictionary page offset in bytes.\nSets list of encodings for this column chunk.\nSets file offset for this row group.\nSets file offset in bytes.\nSets optional file path for this column chunk.\nSets optional index page offset in bytes.\nSets number of rows in this row group.\nSets number of values.\nSets the offset index\nOverride the offset index\nSets optional offset index length in bytes.\nSets optional offset index offset in bytes.\nSets ordinal for this row group.\nSets page encoding stats for this column chunk.\nSets optional repetition level histogram\nSets all the row groups to the specified list\nSets the sorting order for columns\nSets statistics for this column chunk.\nSets total size in bytes for this row group.\nSets total compressed size in bytes.\nSets total uncompressed size in bytes.\nSets optional length of variable length data in bytes.\nReturns the sort ordering of the rows in this RowGroup if …\nReturns statistics that are set for this column chunk, or …\nReturns the current column index from the builder, …\nTakes ownership of the the column metadata in this …\nReturns the current offset index from the builder, …\nTakes ownership of the row groups in this builder, and …\nMethod to convert to Thrift <code>ColumnMetaData</code>\nMark this column index as invalid\nMethod to convert to Thrift.\nMethod to convert to Thrift.\nTotal byte size of all uncompressed column data in this …\nCreates a new level histogram data.\nReturns the total uncompressed data size of this column …\nReturns the number of bytes of variable length data after …\nUpdates histogram values using provided repetition levels\nIs the information in the builder valid?\nIs the information in the builder valid?\nReturns a reference to the the histogram’s values.\nReturns version of this file.\nTrait for calculating the size of various containers\nReturn the size of any bytes allocated on the heap by this …\nReads the <code>ParquetMetaData</code> from a byte stream.\nDecodes the Parquet footer returning the metadata length …\nDecodes <code>ParquetMetaData</code> from the provided bytes.\nSet the column_index and offset_indexes to empty <code>Vec</code> for …\nReturn the parsed <code>ParquetMetaData</code> struct, leaving <code>None</code> in …\nReturns the argument unchanged.\nReturn the number of bytes to read in the initial pass. If …\nIndicates whether this reader has a <code>ParquetMetaData</code> …\nCalls <code>U::from(self)</code>.\nGiven a <code>MetadataFetch</code>, parse and return the <code>ParquetMetaData</code>…\nAsynchronously fetch the page index structures when a …\nCreate a new <code>ParquetMetaDataReader</code>\nCreate a new <code>ParquetMetaDataReader</code> populated with a …\nGiven a <code>ChunkReader</code>, parse and return the <code>ParquetMetaData</code> …\nParses column orders from Thrift definition. If no column …\nRead the page index structures when a <code>ParquetMetaData</code> has …\nRead the page index structures when a <code>ParquetMetaData</code> has …\nAttempts to (asynchronously) parse the footer metadata …\nAttempts to parse the footer metadata (and optionally page …\nSame as <code>Self::try_parse()</code>, but provide the original file …\nEnable or disable reading the Parquet ColumnIndex …\nEnable or disable reading the Parquet OffsetIndex …\nEnable or disable reading the page index structures …\nProvide a hint as to the number of bytes needed to fully …\nWrites <code>ParquetMetaData</code> to a byte stream\nWrites <code>crate::file::metadata</code> structures to a thrift …\nAssembles and writes the final metadata to self.buf\nWrite the metadata to the buffer\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCreate a new <code>ParquetMetaDataWriter</code> to write to <code>buf</code>\nCreate a new ParquetMetaDataWriter to write to <code>buf</code>\nSerialize all the column indexes to the <code>self.buf</code>\nSerialize all the offset indexes to <code>self.buf</code>,\nPageEncodingStats for a column chunk and data page.\nnumber of pages of this type with this encoding\nencoding of the page\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nthe page type (data/dic/…)\nConverts <code>PageEncodingStats</code> into Thrift definition.\nConverts Thrift definition into <code>PageEncodingStats</code>.\n<code>Index</code> structures holding decoded <code>ColumnIndex</code> information\nSupport for reading <code>Index</code> and <code>PageLocation</code> from parquet …\n<code>OffsetIndexMetaData</code> structure holding decoded <code>OffsetIndex</code> …\nBoolean type index\nByte array type index\n64-bit floating point type index\nFixed length byte array type index\n32-bit floating point type index\n32-bit integer type index\n64-bit integer type index\n96-bit integer type (timestamp) index\nStatistics for data pages in a column chunk.\nSometimes reading page index from parquet file will only …\nStrongly typed statistics for data pages in a column chunk.\nThe physical data type of the column\nTyped statistics for one data page\nIf the min/max elements are ordered, and if so in which …\nReturns the definition level histogram for the page\nDefinition level histogram for the page\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet boundary_order of this page index.\nThe actual column indexes, one item per page\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nReturn min/max elements inside ColumnIndex are ordered or …\nReturns the maximum value in the page\nThe maximum value, It is None when all values are null\nReturns the minimum value in the page as bytes\nReturns the minimum value in the page\nThe minimum value, It is None when all values are null\nReturns the maximum value in the page as bytes\nReturns the number of null values in the page\nNull values in the page\nReturns the repetition level histogram for the page\nRepetition level histogram for the page\nCreates a new <code>NativeIndex</code>\nComputes the covering range of two optional ranges\nReads per-column <code>Index</code> for all columns of a row group by …\nReads per-column <code>OffsetIndexMetaData</code> for all columns of a …\nReads <code>OffsetIndex</code>,  per-page <code>PageLocation</code> for all columns …\n<code>OffsetIndex</code> information for a column chunk. Contains …\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nVector of <code>PageLocation</code> objects, one per page in the chunk.\nVector of <code>PageLocation</code> objects, one per page in the chunk.\nCreates a new <code>OffsetIndexMetaData</code> from an <code>OffsetIndex</code>.\nOptional vector of unencoded page sizes, one per page in …\nOptional vector of unencoded page sizes, one per page in …\nWrite Bloom Filters of each row group right after the row …\nWhere in the file <code>ArrowWriter</code> should write Bloom filters\nControls the bloom filter to be computed by the writer.\nCompute column chunk-level statistics but not page-level.\nContainer for column properties that can be changed as …\nDefault value for <code>BloomFilterProperties::fpp</code>\nDefault value for <code>BloomFilterProperties::ndv</code>\nDefault value for <code>WriterProperties::bloom_filter_position</code>\nDefault value for …\nDefault value for <code>WriterProperties::compression</code>\nDefault value for <code>WriterProperties::created_by</code>\nDefault value for …\nDefault value for <code>WriterProperties::dictionary_enabled</code>\nDefault value for …\nDefault value for <code>WriterProperties::max_row_group_size</code>\nDefault value for <code>WriterProperties::max_statistics_size</code>\nDefault value for <code>WriterProperties::data_page_size_limit</code>\nDefault value for <code>WriterProperties::statistics_enabled</code>\nDefault values for …\nDefault value for <code>WriterProperties::writer_version</code>\nDefault value for <code>WriterProperties::write_batch_size</code>\nControls the level of statistics to be computed by the …\nWrite Bloom Filters at the end of the file\nCompute no statistics.\nParquet format version 1.0\nParquet format version 2.0\nCompute page-level and column chunk-level statistics.\nConfiguration settings for reading parquet files.\nBuilder for parquet file reader configuration. See example …\nReference counted reader properties.\nConfiguration settings for writing parquet files.\nBuilder for  <code>WriterProperties</code> parquet writer configuration.\nReference counted writer properties.\nParquet writer version.\nReturns writer version as <code>i32</code>.\nReturns maximum number of rows in a row group.\nReturns the <code>BloomFilterProperties</code> for the given column\nReturns the bloom filter properties, or <code>None</code> if not enabled\nbloom filter related properties\nFinalizes the configuration and returns immutable writer …\nFinalizes the configuration and returns immutable reader …\nReturns builder for reader properties with default values.\nReturns a new default <code>WriterPropertiesBuilder</code> for creating …\nReturns codec options.\nReturns the maximum length of truncated min/max values in …\nReturns compression codec for a column.\nReturns optional compression codec for this column.\nReturns <code>created_by</code> string.\nReturns the maximum page row count\nReturns data page size limit.\nReturns data page size limit.\nReturns encoding for a data page, when dictionary encoding …\nReturns <code>true</code> if dictionary encoding is enabled for a …\nReturns <code>Some(true)</code> if dictionary encoding is enabled for …\nReturns encoding for dictionary page, when dictionary …\nReturns dictionary page size limit.\nReturns dictionary page size limit.\nReturns encoding for a column, if set. In case when …\nReturns optional encoding for this column.\nFalse positive probability, should be always between 0 and …\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nHelper method to get existing or new mutable reference of …\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nReturns <code>key_value_metadata</code> KeyValue pairs.\nReturns maximum number of rows in a row group.\nReturns max size for statistics. Only applicable if …\nReturns optional max size in bytes for statistics.\nNumber of distinct values, should be non-negative to be …\nCreate a new <code>WriterProperties</code> with the default settings\nReturns whether to read bloom filter\nEnable/disable backward compatible LZ4.\nSets if bloom filter is enabled by default for all columns …\nIf <code>value</code> is <code>true</code>, sets bloom filter properties to default …\nSets the default target bloom filter false positive …\nSets the false positive probability for bloom filter for …\nSets default number of distinct values (ndv) for bloom …\nSets the number of distinct (unique) values for bloom …\nSets where in the final file Bloom Filters are written …\nSets whether a bloom filter should be written for a …\nSets the false positive probability for bloom filter for a …\nSets the number of distinct values for bloom filter for a …\nSets compression codec for a specific column.\nSets flag to enable/disable dictionary encoding for a …\nSets encoding for a specific column.\nSets the max length of min/max value fields when writing …\nSets max size for statistics for a specific column.\nSets statistics level for a specific column.\nSets default compression codec for all columns (default to …\nSets compression codec for this column.\nSets “created by” property (defaults to …\nSets best effort maximum number of rows in a data page …\nSets best effort maximum size of a data page in bytes …\nSets best effort maximum size of a data page in bytes.\nSets default flag to enable/disable dictionary encoding …\nSets whether or not dictionary encoding is enabled for …\nSets best effort maximum dictionary page size, in bytes …\nSets best effort maximum dictionary page size, in bytes.\nSets default encoding for all columns.\nSets encoding for this column.\nSets “key_value_metadata” property (defaults to <code>None</code>).\nSets maximum number of rows in a row group (defaults to …\nSets default max statistics size for all columns (defaults …\nSets max size for statistics for this column.\nEnable/disable reading bloom filter\nSets sorting order of rows in the row group if any …\nSets default statistics level for all columns (defaults to …\nSets whether or not statistics are enabled for this column.\nSets the max length of min/max value fields in row group …\nSets write batch size (defaults to 1024).\nSets the <code>WriterVersion</code> written into the parquet metadata …\nReturns sorting columns.\nReturns which statistics are written for a column.\nReturns <code>Some(true)</code> if statistics are enabled for this …\nReturns the maximum length of truncated min/max values in …\nReturns default state of the builder.\nReturns default state of the builder.\nReturns configured batch size for writes.\nReturns configured writer version.\nGenerates <code>Read</code>ers to read chunks of a Parquet data source.\nImplementation of page iterator for parquet file.\nParquet file reader API. With this, user can get metadata …\nLength should return the total number of bytes in the …\nParquet row group reader API. With this, user can get …\nThe concrete type of reader returned by this trait\nReturns the argument unchanged.\nGet a range of data in memory as <code>Bytes</code>\nGet bloom filter for the <code>i</code>th column chunk, if present and …\nGet page reader for the <code>i</code>th column chunk.\nGet value reader for the <code>i</code>th column chunk.\nGet a <code>Read</code> instance starting at the provided file offset\nGet the <code>i</code>th row group reader. Note this doesn’t do bound …\nGet an iterator over the row in this file, see <code>RowIter</code> for …\nGet an iterator over the row in this file, see <code>RowIter</code> for …\nCalls <code>U::from(self)</code>.\nReturns the amount of bytes of the inner source.\nGet metadata information about this file.\nGet metadata information about this row group.\nCreates a page iterator for all row groups in file.\nGet the total number of column chunks in this row group.\nGet the total number of row groups for this file.\nCreate page iterator from parquet file reader with only …\nA predicate for filtering row groups, invoked with the …\nA collection of options for reading a Parquet file.\nA builder for <code>ReadOptions</code>. For the predicates that are …\nA serialized implementation for Parquet <code>FileReader</code>.\nA serialized implementation for Parquet <code>PageReader</code>.\nA serialized implementation for Parquet <code>RowGroupReader</code>.\nSeal the builder and return the read options\nDecodes a <code>Page</code> from the provided <code>buffer</code>\nThe compression codec for this column chunk. Only set for …\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nget bloom filter for the <code>i</code>th column\nGet midpoint offset for a row group\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCreates file reader from a Parquet file. Returns error if …\nCreates new row group reader from a file, row group …\nCreates a new serialized page reader from a chunk reader …\nNew builder\nCreates file reader from a Parquet file with read options. …\nCreates a new serialized page with custom options.\nColumn chunk type.\nReads a <code>PageHeader</code> from the provided <code>Read</code>\nReads a <code>PageHeader</code> from the provided <code>Read</code> returning the …\nThe chunk reader\nEnable reading the page index structures described in “…\nAdd a predicate on row group metadata to the reading …\nAdd a range predicate on filtering row groups if their …\nSet the <code>ReaderProperties</code> configuration.\nRemaining dictionary location if any\nThe current byte offset in the reader\nRemaining page locations\nThe length of the chunk in bytes\nThe total number of rows in this column chunk\nStatistics for Boolean column\nStatistics for ByteArray column\nStatistics for Double column\nStatistics for FixedLenByteArray column\nStatistics for Float column\nStatistics for Int32 column\nStatistics for Int64 column\nStatistics for Int96 column\nStrongly typed statistics for a column chunk within a row …\nTyped implementation for <code>Statistics</code>.\nTyped statistics for one column chunk\nWhether or not min and max values are set. Normally both …\nCreates new statistics for <code>Boolean</code> column type.\nCreates new statistics for <code>ByteArray</code> column type.\nReturns optional value of number of distinct values …\nReturns optional value of number of distinct values …\nReturns optional value of number of distinct values …\nCreates new statistics for <code>Double</code> column type.\nCreates new statistics for <code>FixedLenByteArray</code> column type.\nCreates new statistics for <code>Float</code> column type.\nReturns the argument unchanged.\nReturns the argument unchanged.\nConverts Thrift definition into <code>Statistics</code>.\nWhether or not min and max values are set. Normally both …\nWhether or not min and max values are set. Normally both …\nReturns <code>true</code> if statistics collected any null values, <code>false</code>…\nCreates new statistics for <code>Int32</code> column type.\nCreates new statistics for <code>Int64</code> column type.\nCreates new statistics for <code>Int96</code> column type.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nOld versions of parquet stored statistics in <code>min</code> and <code>max</code> …\nOld versions of parquet stored statistics in <code>min</code> and <code>max</code> …\nIf <code>true</code> the statistics are compatible with the deprecated …\nIf <code>true</code> the statistics are compatible with the deprecated …\nReturns <code>true</code> if statistics have old <code>min</code> and <code>max</code> fields set.\nReturns <code>true</code> if statistics were created using old min/max …\nIf <code>true</code> populate the deprecated <code>min</code> and <code>max</code> fields instead …\nIf <code>true</code> populate the deprecated <code>min</code> and <code>max</code> fields instead …\nReturns max value of the statistics.\nReturns slice of bytes that represent max value. Panics if …\nReturns max value as bytes of the statistics.\nReturns slice of bytes that represent max value, if max …\nReturns max value as bytes of the statistics, if max value …\nReturns <code>true</code> if the max value is set, and is an exact max …\nWhether or not max value is set, and is an exact value.\nReturns max value of the statistics, if known.\nReturns min value of the statistics.\nReturns slice of bytes that represent min value. Panics if …\nReturns min value as bytes of the statistics.\nReturns slice of bytes that represent min value, if min …\nReturns min value as bytes of the statistics, if min value …\nReturns <code>true</code> if the min value is set, and is an exact min …\nWhether or not min value is set, and is an exact value.\nReturns min value of the statistics, if known.\nCreates new statistics for a column type\nCreates new typed statistics.\nReturns number of null values for the column. Note that …\nReturns number of null values for the column. Note that …\nReturns number of null values for the column, if known. …\nReturns null count.\nReturns physical type associated with statistics.\nMacro to generate methods to create Statistics.\nConvert Statistics into Thrift definition.\nSet whether to write the deprecated <code>min</code> and <code>max</code> fields for …\nSet whether the stored <code>max</code> field represents the exact …\nSet whether the stored <code>min</code> field represents the exact …\nCallback invoked on closing a column chunk\nCallback invoked on closing a row group, arguments are:\nA wrapper around a <code>ColumnWriter</code> that invokes a callback on …\nParquet file writer API. Provides methods to write row …\nA serialized implementation for Parquet <code>PageWriter</code>. Writes …\nParquet row group writer API. Provides methods to access …\nA wrapper around a <code>Write</code> that keeps track of the number of …\nAppend an encoded column chunk from another source without …\nAdd a <code>KeyValue</code> to the file writer’s metadata\nReturns the number of bytes written to this instance\nReturns the number of bytes written to this instance\nCloses and finalises file writer, returning the file …\nCloses this row group writer and returns row group …\nClose this <code>SerializedColumnWriter</code>\nClose and finalize the underlying Parquet writer\nReturns metadata for any flushed row groups\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns <code>OnCloseColumnChunk</code> for the next writer\nReturns a reference to the underlying writer.\nReturns a reference to the underlying writer.\nReturns a mutable reference to the underlying writer.\nReturns a mutable reference to the underlying writer.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nReturns the underlying writer.\nWrites the file footer and returns the underlying writer.\nCreate a new <code>TrackedWrite</code> from a <code>Write</code>\nCreates new file writer.\nCreates a new <code>SerializedRowGroupWriter</code> with:\nCreate a new <code>SerializedColumnWriter</code> from a <code>ColumnWriter</code> …\nCreates new page writer.\nReturns the next column writer, if available; otherwise …\nAdvance <code>self.column_index</code> returning the next <code>ColumnDescPtr</code> …\nReturns the next column writer, if available, using the …\nCreates new row group from this file writer. In case of IO …\nReturns a reference to the writer properties\nReturns a reference to schema descriptor.\nSerializes page header into Thrift. Returns number of …\nWrites magic bytes at the beginning of the file.\nReturns a reference to a typed <code>ColumnWriterImpl</code>\nReturns a reference to an untyped <code>ColumnWriter</code>\nSerialize all the bloom filters of the given row group to …\nAssembles and writes metadata at the end of the file.\nBit packed encoding.  This can only be used if the data …\nAn embedded BSON document\nEncoding for fixed-width data (FLOAT, DOUBLE, INT32, …\nBloom filter header is stored at beginning of Bloom filter …\nEnum to annotate whether lists of min/max elements inside …\nEmbedded BSON logical type annotation\nOptional statistics for each data page in a ColumnChunk.\nDescription for column metadata\nSupported compression algorithms.\nDEPRECATED: Common types used by frameworks(e.g. hive, …\nA Date\nA decimal value.\nDelta encoding for integers. This can be used for int …\nIncremental-encoded byte array. Prefix lengths are encoded …\nEncoding for byte arrays to separate the length values and …\nData page header\nNew page format allowing reading levels without …\nDecimal logical type annotation\nThe dictionary page must be placed at the first position …\nan enum is converted into a BYTE_ARRAY field\nEncodings supported by Parquet.  Not all encodings are …\nRepresentation of Schemas\nCrypto metadata for files with encrypted footer *\nDescription for file metadata\nAn interval of time\nA signed integer value.\nInteger logical type annotation\nAn embedded JSON document\nEmbedded JSON logical type annotation\nWrapper struct to store key values\na list is converted into an optional field containing a …\na map is converted as an optional field containing a …\na key/value pair is converted into a group of two fields\nTime units for logical types\nLogical type to annotate a column that is always null.\nThe field is optional (can be null) and each row has 0 or …\nOptional offsets for each data page in a ColumnChunk.\nDefault encoding. BOOLEAN - 1 bit per value. 0 is false; 1 …\nDeprecated: Dictionary encoding. The values in the …\nstatistics of a given page type and encoding\nThe field is repeated and can contain 0 or more values\nThis field is required (can not be null) and each row has …\nGroup packed run length encoding. Usable for …\nDictionary encoding: the ids are encoded using the RLE …\nRepresents a element inside a schema definition.\nA structure for capturing metadata for estimating the …\nSort order within a RowGroup of a leaf column\nBlock-based algorithm type annotation. *\nStatistics per row group and per page All fields are …\nEmpty structs to use as logical type annotations\nA date/time combination\nA date/time combination\nA time.\nA time\nTime logical type annotation\nTimestamp logical type annotation\nTypes supported by Parquet.  These types are intended to …\nEmpty struct to signal the order defined by the physical …\nAn unsigned integer value.\na BYTE_ARRAY actually contains UTF8 encoded chars\nThe compression used in the Bloom filter.\nHash strategy type annotation. xxHash is an extremely fast …\nUnique file identifier part of AAD suffix *\nUnique file identifier part of AAD suffix *\nAAD prefix *\nAAD prefix *\nThe algorithm for setting bits. *\nSize of Bloom filter data including the serialized header, …\nByte offset from beginning of file to Bloom filter data. *\nStores whether both min_values and max_values are ordered …\nCompression codec *\nThe ordinal position of the column (in this row group) *\nSize of ColumnChunk’s ColumnIndex, in bytes *\nFile offset of ColumnChunk’s ColumnIndex *\nSort order used for the min_value and max_value fields in …\nMetadata for each column chunk in this row group. This …\nCompressed (and potentially encrypted) page size in bytes, …\nSize of the page, including header. Sum of …\nThe compression used in the Bloom filter *\nDEPRECATED: When the schema is the result of a conversion …\nnumber of pages of this type with this encoding *\nThe 32-bit CRC checksum for the page, to be be calculated …\nString for application that wrote this file.  This should …\nCrypto metadata of encrypted columns *\nByte offset from beginning of file to first data page *\nEncoding used for definition levels *\nSame as repetition_level_histogram except for definition …\nSame as repetition_level_histograms except for definitions …\nLength of the definition levels\nIf true, indicates this column is sorted in descending …\nByte offset from the beginning of file to first (only) …\ncount of distinct values occurring\nEncoding used for this data page *\nEncoding using this dictionary page *\nEncoding used for data in this page *\nencoding of the page *\nSet of all encodings used for pages in this column chunk. …\nSet of all encodings used for this column. The purpose is …\nEncrypted column metadata for this chunk *\nEncryption algorithm. This field is set only in encrypted …\nEncryption algorithm. This field is only used for files …\nWhen the original schema supports field ids, this will …\nDeprecated: Byte offset in file_path to the ColumnMetaData\nByte offset from beginning of file to first page (data or …\nFile where column data is stored.  If not set, assumed to …\nIndex within the RowGroup of the first row of the page. …\nRetrieval metadata of key used for signing the footer. …\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nThe hash function used for Bloom filter. *\nByte offset from beginning of file to root index page *\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nWhether the values are compressed. Which means the section …\nIf true, max_value is the actual maximum value for a column\nIf true, min_value is the actual minimum value for a column\nIf true, the entries in the dictionary are sorted in …\nRetrieval metadata of column encryption key *\nRetrieval metadata of key used for encryption of footer, …\nOptional key/value metadata *\nOptional key/value metadata *\nThe logical type of this SchemaElement\nDEPRECATED: min and max value of the column. Use min_value …\nLower and upper bound values for the column, determined by …\nColumn metadata for this chunk. Some writers may also …\nTwo lists containing lower and upper bounds for the values …\nName of the field in the schema\ncount of null value in the column\nA list containing the number of null values for each page *\nA list of Boolean values to determine the validity of the …\nIf true, nulls will come before non-null values, otherwise,\nThe size of bitset in bytes *\nNested fields.  Since thrift does not support nested …\nNumber of NULL values, in this data page. Number of …\nNumber of rows in this data page. Every page must begin at …\nNumber of rows in this row group *\nNumber of rows in this file *\nNumber of values, including NULLs, in this data page.\nNumber of values in the dictionary *\nNumber of values, including NULLs, in this data page. *\nNumber of values in this column *\nOffset of the page in the file *\nSize of ColumnChunk’s OffsetIndex, in bytes *\nFile offset of ColumnChunk’s OffsetIndex *\nRow group ordinal in the file *\nPageLocations, ordered by increasing PageLocation.offset. …\nthe page type (data/dic/…) *\nPath in schema *\nColumn path in schema *\nEncoding used for repetition levels *\nWhen present, there is expected to be one element …\nContains repetition level histograms for each page …\nLength of the repetition levels\nrepetition of the field. The root of the schema does not …\nRow groups in this file *\nDEPRECATED: Used when this column contains decimal data. …\nParquet schema for this file.  This schema contains …\nOptional statistics to help estimate total memory when …\nIf set, specifies a sort ordering of the rows in this …\nOptional statistics for the data in this page *\nOptional statistics for the data in this page *\noptional statistics for this column chunk\nIn files encrypted with AAD prefix without storing it, …\nIn files encrypted with AAD prefix without storing it, …\nTotal byte size of all the uncompressed column data in …\ntotal byte size of all compressed, and potentially …\nTotal byte size of all compressed (and potentially …\ntotal byte size of all uncompressed pages in this column …\nData type for this field. Not set if the current element …\nthe type of the page: indicates which of the *_header …\nType of this column *\nIf type is FIXED_LEN_BYTE_ARRAY, this is the byte length …\nUncompressed page size in bytes (not including this …\nUnencoded/uncompressed size for BYTE_ARRAY types.\nThe number of physical bytes stored for BYTE_ARRAY data …\nVersion of this file *\nBoolean value (<code>true</code>, <code>false</code>).\nSigned integer INT_8.\nGeneral binary value.\nDate without a time of day, stores the number of days from …\nDecimal value.\nIEEE 64-bit floating point value.\nAPI to represent a single field in a <code>Row</code>.\nIEEE 32-bit floating point value.\nIEEE 16-bit floating point value.\nStruct, child elements are tuples of field-value pairs.\nSigned integer INT_32.\n<code>List</code> represents a list which contains an array of elements.\nTrait for type-safe access of an index for a <code>List</code>. Note …\nList of elements.\nSigned integer INT_64.\n<code>Map</code> represents a map which contains a list of key-&gt;value …\nTrait for type-safe access of an index for a <code>Map</code>\nList of key-value pairs.\nNull value.\nRead up to <code>num_records</code> records from <code>row_group_reader</code> into …\nTrait describing how to write a record (the implementator) …\n<code>Row</code> represents a nested Parquet record.\nTrait for type-safe convenient access to fields within a …\n<code>RowColumnIter</code> represents an iterator over column names and …\nTrait for formatting fields within a Row.\nSigned integer INT_16.\nUTF-8 encoded character string.\nMicroseconds from the Unix epoch, 1 January 1970.\nMilliseconds from the Unix epoch, 1 January 1970.\nUnsigned integer UINT_8.\nUnsigned integer UINT_32.\nUnsigned integer UINT_64.\nUnsigned integer UINT_16.\nContains Row enum that is used to represent record in Rust.\nThe method to format a field at the given index.\nTry to get a boolean value at the given index.\nTry getting a <code>boolean</code> value at the given index.\nTry to get a byte value at the given index.\nTry getting a <code>byte</code> value at the given index.\nTry to get a bytes value at the given index.\nTry getting a <code>bytes</code> value at the given index.\nTry to get a decimal value at the given index.\nTry getting a <code>decimal</code> value at the given index.\nTry to get a double value at the given index.\nTry getting a <code>f64</code> value at the given index.\nTry to get a float value at the given index.\nTry getting a <code>f32</code> value at the given index.\nTry to get a float16 value at the given index.\nTry getting a <code>f16</code> value at the given index.\nTry to get a group value at the given index.\nTry getting a <code>group</code> value at the given index.\nTry to get a int value at the given index.\nTry getting an <code>i32</code> value at the given index.\nGet the keys of the map.\nTry to get a list value at the given index.\nTry getting a <code>list</code> value at the given index.\nTry to get a long value at the given index.\nTry getting an <code>i64</code> value at the given index.\nTry to get a map value at the given index.\nTry getting a <code>map</code> value at the given index.\nTry to get a short value at the given index.\nTry getting an <code>i16</code> value at the given index.\nTry to get a string value at the given index.\nTry getting a <code>string</code> value at the given index.\nTry to get a date value at the given index.\nTry getting a <code>timestamp</code> as microseconds value encoded as …\nTry to get a date value at the given index.\nTry getting a <code>timestamp</code> as milliseconds value encoded as …\nTry to get a ubyte value at the given index.\nTry getting a <code>u8</code> value at the given index.\nTry to get a uint value at the given index.\nTry getting a <code>u32</code> value at the given index.\nTry to get a ulong value at the given index.\nTry getting a <code>u64</code> value at the given index.\nTry to get a ushort value at the given index.\nTry getting a <code>u16</code> value at the given index.\nGet the values of the map.\nRead up to <code>num_records</code> records from <code>row_group_reader</code> into …\nContains implementation of record assembly and converting …\nGenerated schema used by <code>row_group_writer</code>\nWrites from <code>self</code> into <code>row_group_writer</code>.\nBoolean value (<code>true</code>, <code>false</code>).\nSigned integer INT_8.\nGeneral binary value.\nDate without a time of day, stores the number of days from …\nDecimal value.\nIEEE 64-bit floating point value.\nAPI to represent a single field in a <code>Row</code>.\nIEEE 32-bit floating point value.\nIEEE 16-bit floating point value.\nStruct, child elements are tuples of field-value pairs.\nSigned integer INT_32.\n<code>List</code> represents a list which contains an array of elements.\nTrait for type-safe access of an index for a <code>List</code>. Note …\nList of elements.\nSigned integer INT_64.\n<code>Map</code> represents a map which contains a list of key-&gt;value …\nTrait for type-safe access of an index for a <code>Map</code>\nList of key-value pairs.\nNull value.\n<code>Row</code> represents a nested Parquet record.\nTrait for type-safe convenient access to fields within a …\n<code>RowColumnIter</code> represents an iterator over column names and …\nTrait for formatting fields within a Row.\nSigned integer INT_16.\nUTF-8 encoded character string.\nMicroseconds from the Unix epoch, 1 January 1970.\nMilliseconds from the Unix epoch, 1 January 1970.\nUnsigned integer UINT_8.\nUnsigned integer UINT_32.\nUnsigned integer UINT_64.\nUnsigned integer UINT_16.\nConverts Parquet BOOLEAN type with logical type into <code>bool</code> …\nConverts Parquet BYTE_ARRAY type with converted type into …\nHelper method to convert Parquet date into a string. Input …\nHelper method to convert Parquet decimal into a string. We …\nConverts Parquet DOUBLE type with converted type into <code>f64</code> …\nConverts Parquet FLOAT type with logical type into <code>f32</code> …\nConverts Parquet INT32 type with converted type into <code>i32</code> …\nConverts Parquet INT64 type with converted type into <code>i64</code> …\nConverts Parquet INT96 (nanosecond timestamps) type and …\nHelper method to convert Parquet timestamp into a string. …\nHelper method to convert Parquet timestamp into a string. …\nHelper method to convert Parquet timestamp into a string. …\nGet the reference to the elements in this list\nGet the reference to the key-value pairs in this map\nThe method to format a field at the given index.\nGet Display reference for a given field.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nTry to get a boolean value at the given index.\nTry getting a <code>boolean</code> value at the given index.\nTry to get a byte value at the given index.\nTry getting a <code>byte</code> value at the given index.\nTry to get a bytes value at the given index.\nTry getting a <code>bytes</code> value at the given index.\nGet an iterator to go through all columns in the row.\nTry to get a decimal value at the given index.\nTry getting a <code>decimal</code> value at the given index.\nTry to get a double value at the given index.\nTry getting a <code>f64</code> value at the given index.\nTry to get a float value at the given index.\nTry getting a <code>f32</code> value at the given index.\nTry to get a float16 value at the given index.\nTry getting a <code>f16</code> value at the given index.\nTry to get a group value at the given index.\nTry getting a <code>group</code> value at the given index.\nTry to get a int value at the given index.\nTry getting an <code>i32</code> value at the given index.\nGet the keys of the map.\nTry to get a list value at the given index.\nTry getting a <code>list</code> value at the given index.\nTry to get a long value at the given index.\nTry getting an <code>i64</code> value at the given index.\nTry to get a map value at the given index.\nTry getting a <code>map</code> value at the given index.\nTry to get a short value at the given index.\nTry getting an <code>i16</code> value at the given index.\nTry to get a string value at the given index.\nTry getting a <code>string</code> value at the given index.\nTry to get a date value at the given index.\nTry getting a <code>timestamp</code> as microseconds value encoded as …\nTry to get a date value at the given index.\nTry getting a <code>timestamp</code> as milliseconds value encoded as …\nGet the type name.\nTry to get a ubyte value at the given index.\nTry getting a <code>u8</code> value at the given index.\nTry to get a uint value at the given index.\nTry getting a <code>u32</code> value at the given index.\nTry to get a ulong value at the given index.\nTry getting a <code>u64</code> value at the given index.\nTry to get a ushort value at the given index.\nTry getting a <code>u16</code> value at the given index.\nGet the values of the map.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nMove columns data out of the row. Useful to avoid internal …\nDetermines if this Row represents a primitive value.\nGet the number of fields in this row.\nGet the number of fields in this row\nGet the number of fields in this row\nMacro to generate type-safe get_xxx methods for reference …\nMacro to generate type-safe get_xxx methods for primitive …\nConstructs a <code>List</code> from the list of <code>fields</code> and returns it.\nConstructs a <code>Map</code> from the list of <code>entries</code> and returns it.\nConstructs a <code>Row</code> from the list of <code>fields</code> and returns it.\nMacro to generate type-safe get_xxx methods for primitive …\nMacro as a shortcut to generate ‘not yet implemented’ …\nMacro to generate type-safe get_xxx methods for reference …\nMacro to generate type-safe get_xxx methods for primitive …\nConverts the row into a JSON object.\nConverts the Parquet field into a JSON <code>Value</code>.\nDefault batch size for a reader\nThe enum Either with variants That represents a reference …\nGroup (struct) reader with type information, definition …\nReader of key-value pairs, e.g. maps, contains type …\nOptional reader with definition level of a parent and a …\nPrimitive reader with type information and triplet iterator\nReader tree for record assembly\nInternal iterator of <code>Row</code>s for a reader.\nReader for repeated values, e.g. lists, contains type …\nAccess parquet data as an iterator of <code>Row</code>\nTree builder for <code>Reader</code> enum. Serves as a container of …\nAdvances leaf columns for the current reader.\nCreates iterator of <code>Row</code>s directly from schema descriptor …\nCreates new root reader for provided schema and row group.\nReturns current definition level, Method does not advance …\nReturns current repetition level. Method does not advance …\nReturns field name for the current reader.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCreates iterator of <code>Row</code>s for all row groups in a file.\nCreates a iterator of <code>Row</code>s from a <code>FileReader</code> using the …\nCreates iterator of <code>Row</code>s for a specific row group.\nHelper method to get schema descriptor for projected …\nReturns true, if current reader has more values, false …\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nReturns true if repeated type is an element type for the …\nCreates new tree builder with default parameters.\nCreates a new iterator of <code>Row</code>s.\nWraps reader in option reader based on repetition.\nTries to create a iterator of <code>Row</code>s using projections. …\nReads current record as <code>Row</code> from the reader tree. …\nReads current record as <code>Field</code> from the reader tree. …\nBuilds tree of readers for the current schema recursively.\nReturns repetition for the current reader.\nReturns common tree builder, so the same settings are …\nSets batch size for this tree builder.\nSets batch size for this row iter.\nRead up to <code>num_records</code> records from <code>row_group_reader</code> into …\nRead up to <code>num_records</code> records from <code>row_group_reader</code> into …\nTrait describing how to write a record (the implementator) …\nGenerated schema used by <code>row_group_writer</code>\nWrites from <code>self</code> into <code>row_group_writer</code>.\nHigh level API wrapper on column reader. Provides …\nInternal typed triplet iterator as a wrapper for column …\nReturns column descriptor reference for the current typed …\nReturns current definition level for a leaf triplet …\nReturns current definition level. If field is required, …\nReturns current repetition level for a leaf triplet …\nReturns current repetition level. If field is required, …\nUpdates non-null value for current row.\nReturns current value. Method does not advance the …\nReturns the argument unchanged.\nReturns the argument unchanged.\nProvides check on values/levels left without invoking the …\nQuick check if iterator has more values/levels to read. It …\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nReturns true, if current value is null. Based on the fact …\nReturns max definition level for a leaf triplet iterator\nReturns maximum definition level for the triplet iterator …\nReturns max repetition level for a leaf triplet iterator\nReturns maximum repetition level for the triplet iterator …\nCreates new triplet for column reader\nCreates new typed triplet iterator based on provided …\nInvokes underlying typed triplet iterator to buffer …\nAdvances to the next triplet. Returns true, if there are …\nMacro to generate simple functions that cover all types of …\nParquet schema parser. Provides methods to parse and …\nParquet schema printer. Provides methods to print Parquet …\nContains structs and methods to build Parquet schema and …\nUtilities to traverse against various parquet type.\nInternal Schema parser. Traverses message type using …\nTokenizer to split message type string into tokens that …\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nParses message type as string into a Parquet <code>Type</code> which, …\nSplits string into tokens; input string can already be …\nStruct for printing Parquet message type.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nPrints file metadata <code>FileMetaData</code> information.\nPrints Parquet metadata <code>ParquetMetaData</code> information.\nPrints Parquet <code>Type</code> information.\nBasic type info. This contains information such as the …\nType alias for <code>Arc&lt;ColumnDescriptor&gt;</code>.\nPhysical type for leaf-level primitive columns.\nRepresents the location of a column in a Parquet schema\nRepresents a group of fields (similar to struct).\nA builder for group types. All attributes are optional …\nRepresents a primitive leaf field.\nA builder for primitive types. All attributes are optional …\nType alias for <code>Arc&lt;SchemaDescriptor&gt;</code>.\nSchema of a Parquet file.\nRepresentation of a Parquet type.\nType alias for <code>Arc&lt;Type&gt;</code>.\nAppends more components to end of column path.\nCreates a new <code>PrimitiveType</code> instance from the collected …\nCreates a new <code>GroupType</code> instance from the gathered …\nChecks if <code>sub_type</code> schema is part of current schema. This …\nReturns <code>ColumnDescriptor</code> for a field position.\nReturns slice of <code>ColumnDescriptor</code>.\nReturns <code>ConvertedType</code> value for the type.\nReturns <code>ConvertedType</code> for this column.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nMethod to convert from Thrift.\nConstructs a new Type from the <code>elements</code>, starting at index …\nReturns <code>BasicTypeInfo</code> information about the type.\nReturns column root <code>Type</code> for a leaf position.\nReturns the index of the root column for a field position\nReturns column root <code>Type</code> pointer for a leaf position.\nGets the fields from this group type. Note that this will …\nGets physical type of this primitive type. Note that this …\nGets precision of this primitive type. Note that this will …\nGets scale of this primitive type. Note that this will …\nCreates group type builder with provided column name.\nReturns <code>true</code> if id is set, <code>false</code> otherwise.\nReturns <code>true</code> if type has repetition field set, <code>false</code> …\nReturns id value for the type.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nReturns <code>true</code> if this type is a group type, <code>false</code> otherwise.\nReturns <code>true</code> if this type is repeated or optional. If this …\nReturns <code>true</code> if this type is a primitive type, <code>false</code> …\nReturns <code>true</code> if this type is the top-level schema type …\nMapping from a leaf column’s index to the root column …\nThe descriptors for the physical type of each leaf column …\nReturns <code>LogicalType</code> value for the type.\nReturns <code>LogicalType</code> for this column.\nReturns maximum definition level for this column.\nThe maximum definition level for this column\nReturns maximum repetition level for this column.\nThe maximum repetition level for this column\nReturns this type’s field name.\nReturns field name.\nReturns column name.\nReturns schema name.\nCreates new primitive type builder with provided field …\nCreates new group type builder with provided field name.\nCreates new column path from vector of field names.\nCreates new descriptor for leaf-level column.\nCreates new schema descriptor from Parquet schema.\nReturns number of leaf-level columns.\nReturns a slice of path components.\nReturns <code>ColumnPath</code> for this column.\nThe path of this column. For instance, “a.b.c.d”.\nReturns physical type for this column. Note that it will …\nThe “leaf” primitive type of this column\nCreates primitive type builder with provided field name …\nReturns <code>Repetition</code> value for the type.\nReturns schema as <code>Type</code>.\nReturns schema as <code>TypePtr</code> for cheap cloning.\nThe top-level logical schema (the “message” type).\nReturns self type <code>Type</code> for this leaf column.\nReturns self type <code>TypePtr</code>  for this leaf column.\nReturns the sort order for this column\nReturns string representation of this column path.\nMethod to convert to Thrift.\nConstructs list of <code>SchemaElement</code> from the schema using …\nReturns type length for this column. Note that it will …\nReturns type precision for this column. Note that it will …\nReturns type scale for this column. Note that it will …\nSets <code>ConvertedType</code> for this field and returns itself.\nSets <code>ConvertedType</code> for this field and returns itself.\nSets a list of fields that should be child nodes of this …\nSets optional field id and returns itself.\nSets optional field id and returns itself.\nSets type length and returns itself. This is only applied …\nSets <code>LogicalType</code> for this field and returns itself. If …\nSets <code>LogicalType</code> for this field and returns itself.\nSets precision for Parquet DECIMAL physical type and …\nSets <code>Repetition</code> for this field and returns itself.\nSets <code>Repetition</code> for this field and returns itself.\nSets scale for Parquet DECIMAL physical type and returns …\nBasic information about the type.\nBasic information about the type.\nFields of this group type.\nPhysical type of this primitive type.\nPrecision of this type.\nScale of this type.\nLength of this type.\nA utility trait to help user to traverse against parquet …\nA utility method which detects input type and calls …\nDefault implementation when visiting a list.\nCalled by <code>visit_list</code>.\nCalled when a map type hit.\nCalled when a primitive type hit.\nCalled when a struct type hit.\nWrite messages using the Thrift compact protocol.\nA more performant implementation of <code>TCompactInputProtocol</code> …\nReads and writes the struct to Thrift protocols.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCreate a <code>TCompactOutputProtocol</code> that writes bytes to …\nReads the struct from the input Thrift protocol\nWrites the struct to the output Thrift protocol")